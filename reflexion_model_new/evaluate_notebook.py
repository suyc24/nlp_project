import os
import re
import torch
import numpy as np
import time
import argparse
import yaml
from collections import Counter
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from vllm import LLM, SamplingParams
from tqdm import tqdm

# ç¡®ä¿ç¼“å­˜è·¯å¾„æ­£ç¡®
HF_CACHE_DIR = "/root/autodl-tmp/hf_cache"
os.makedirs(HF_CACHE_DIR, exist_ok=True)
os.environ["HF_HOME"] = HF_CACHE_DIR

# å¤ç”¨ Principle Manager ä»¥ç¡®ä¿æ£€ç´¢é€»è¾‘ä¸€è‡´
from principle_manager import MemoryManager

class ScientificComparator:
    def __init__(self, config):
        print(f"ğŸš€ åˆå§‹åŒ–è¯„ä¼°å¼•æ“ (Adaptive RAG Mode)...")
        self.config = config
        self.MODEL_PATH = config["MODEL_PATH"]
        self.GPU_UTILIZATION = config["GPU_UTILIZATION"]
        self.TOP_K = config["TOP_K"]
        
        # vLLM åˆå§‹åŒ–
        self.llm = LLM(
            model=self.MODEL_PATH, 
            trust_remote_code=True,
            gpu_memory_utilization=self.GPU_UTILIZATION,
            tensor_parallel_size=1, 
            max_model_len=2048,
            download_dir=HF_CACHE_DIR
        )
        
        # 1. æŠ½è±¡å‚æ•° (Greedy Decode - å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´)
        self.params_abstract = SamplingParams(
            temperature=0.0, 
            top_p=0.9, 
            max_tokens=128,
            stop=["<|im_end|>", "<|endoftext|>"]
        )

        # 2. ç”Ÿæˆå‚æ•° (High Temp + Majority Vote)
        # é‡‡æ · 3 æ¬¡ä»¥è®¡ç®—ä¸€è‡´æ€§
        self.params_generate = SamplingParams(
            n=3, 
            temperature=0.2, 
            top_p=0.95, 
            max_tokens=1024,
            stop=["<|im_end|>", "<|endoftext|>"]
        )
        
        # 3. Baseline ç”Ÿæˆå‚æ•° (n=6)
        self.params_generate_baseline = SamplingParams(
            n=6, 
            temperature=0.2, 
            top_p=0.95, 
            max_tokens=1024,
            stop=["<|im_end|>", "<|endoftext|>"]
        )

        print("ğŸ“¥ åŠ è½½ Embedder (CPU)...")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device="cpu", cache_folder=HF_CACHE_DIR)
        
        # åŠ è½½è®­ç»ƒå¥½çš„è®°å¿†åº“
        self.memory = MemoryManager(reset=False)

    def construct_abstraction_prompt(self, q):
        """
        [å…³é”®] å¿…é¡»ä¸ evolution_trainer.py å®Œå…¨ä¸€è‡´ï¼Œä¿è¯ Trigger ç›¸åŒ
        """
        content = f"""
            Task: Identify the core mathematical concept and intent of the following problem.
            Output a concise, abstract description of the problem and its condition.
            ### Requirements
            - **Format**: Your output must be a single sentence following this pattern: "[Abstract Problem Type] given that [Specific Conditions from the Question including numerical constraints, relationships, and constraints]"
            - **Strict Constraint**: Do NOT include any specific numbers (e.g., 16, 3) or specific nouns (e.g., eggs, ducks) from the current problem. The principle must be universal. 

            [Example]
            Q: John has 5 apples and buys 3 more. How many?
            A: Calculating the total sum of objects given that each part is provided.

            [Target]
            Q: {q}
            A:"""
        return f"<|im_start|>user\n{content}<|im_end|>\n<|im_start|>assistant\n"

    def construct_prompt(self, q, context=None):
        """
        æ ‡å‡†æ¨ç† Prompt
        """
        if context:
            content = f"""
[Reference Rules]
{context}

[Question]
{q}

[Instruction]
1. Read the Reference Rules carefully.
2. First, decide which rule is relevant to the question.
3. If a rule is relevant, write "Selected Rule: [Rule Content]".
4. If no rule is relevant, write "No suitable rule found".
5. Then, solve the problem step-by-step using the selected rule (if any).

Answer:"""
        else:
            content = f"Question: {q}\nAnswer step-by-step:"
            
        return f"<|im_start|>user\n{content}<|im_end|>\n<|im_start|>assistant\n"

    def extract_answer(self, text):
        if not text: return None
        # ç§»é™¤é€—å·ä»¥ä¾¿è§£ææ•°å­— (e.g., 1,000 -> 1000)
        text = text.replace(',', '')
        # æå–æœ€åä¸€ä¸ªæ•°å­—
        matches = re.findall(r'-?\d+\.?\d*', text)
        if matches: return float(matches[-1])
        return None

    def get_majority_vote(self, request_output):
        """
        è¿”å› (ä¼—æ•°ç­”æ¡ˆ, ä¼—æ•°å‡ºç°çš„æ¬¡æ•°, æ‰€æœ‰æœ‰æ•ˆç­”æ¡ˆåˆ—è¡¨)
        """
        valid_nums = []
        for output in request_output.outputs:
            num = self.extract_answer(output.text)
            if num is not None:
                valid_nums.append(num)
        
        if not valid_nums: return None, 0, []
        
        # æ‰¾åˆ°å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç­”æ¡ˆ
        counter = Counter(valid_nums)
        most_common = counter.most_common(1)[0] # (value, count)
        return most_common[0], most_common[1], valid_nums

    def check_correct(self, pred, gt_str):
        if "####" in gt_str:
            gold = self.extract_answer(gt_str.split("####")[1])
        else:
            gold = self.extract_answer(gt_str)
        
        if pred is None or gold is None: return False
        return abs(pred - gold) < 1e-4

    def batch_generate_vllm(self, prompts, sampling_params):
        outputs = self.llm.generate(prompts, sampling_params, use_tqdm=True)
        return outputs

    def run_scientific_test(self):
        # åŠ è½½æµ‹è¯•é›†
        dataset = load_dataset("gsm8k", "main")['test']
        # ä¸ºäº†å¿«é€Ÿæ¼”ç¤ºï¼Œè¿™é‡Œå¯ä»¥åˆ‡ç‰‡ï¼Œå…¨é‡æµ‹è¯•è¯·å»æ‰åˆ‡ç‰‡
        # dataset = dataset.select(range(200)) 
        questions = dataset['question']
        ground_truths = dataset['answer']
        total = len(questions)
        
        print(f"ğŸ“Š Test Set Size: {total}")
        print(f"âš™ï¸  Settings: Temp=1.0, N=3 (Majority Vote)")

        # ======================================================================
        # Phase 1: Baseline (No RAG) - 6æ¬¡é‡‡æ · (å…¬å¹³å¯¹æ¯”)
        # ======================================================================
        print(f"\nğŸ”µ [Group A] Baseline (No RAG, n=6)...")
        base_prompts = [self.construct_prompt(q, context=None) for q in questions]
        base_outputs = self.batch_generate_vllm(base_prompts, self.params_generate_baseline)

        # ======================================================================
        # Phase 2: RAG (With Abstraction) - 3æ¬¡é‡‡æ · (ç”¨äº Adaptive ç»„åˆ)
        # ======================================================================
        print(f"\nğŸŸ¢ [Group B] RAG (With Abstraction)...")
        
        # 1. Abstract (Greedy)
        print("   ğŸ§  Step 1: Abstracting questions (Greedy)...")
        abstract_prompts = [self.construct_abstraction_prompt(q) for q in questions]
        abstract_outputs = self.batch_generate_vllm(abstract_prompts, self.params_abstract)
        abstract_queries = [out.outputs[0].text.strip() for out in abstract_outputs]

        # 2. Retrieve
        print("   ğŸ” Step 2: Retrieving rules...")
        query_embeddings = self.embedder.encode(abstract_queries, batch_size=64, show_progress_bar=True, convert_to_numpy=True).tolist()
        retrieved_batch = self.memory.batch_retrieve(query_embeddings, top_k=self.TOP_K, threshold=0.0)

        # 3. Generate RAG
        rag_prompts = []
        for i, q in enumerate(questions):
            rules_list = retrieved_batch[i]
            if rules_list:
                context_text = "\n".join([f"[Rule {k+1}]: {r[0]}" for k, r in enumerate(rules_list)])
                rag_prompts.append(self.construct_prompt(q, context_text))
            else:
                rag_prompts.append(self.construct_prompt(q, context=None))

        print("   âœï¸  Step 3: Generating RAG answers...")
        rag_outputs = self.batch_generate_vllm(rag_prompts, self.params_generate)

        # ======================================================================
        # Phase 3: Evaluation & Adaptive Selection
        # ======================================================================
        correct_base = 0
        correct_pure_rag = 0
        correct_adaptive = 0
        
        adaptive_log = []

        print("\nâš–ï¸  Calculating Metrics...")
        for i in range(total):
            gt_str = ground_truths[i]
            
            # 1. è§£æ Baseline (n=6)
            base_ans, base_count, base_list = self.get_majority_vote(base_outputs[i])
            base_is_correct = self.check_correct(base_ans, gt_str)
            if base_is_correct: correct_base += 1

            # 2. è§£æ RAG (n=3)
            rag_ans, rag_count, rag_list = self.get_majority_vote(rag_outputs[i])
            rag_is_correct = self.check_correct(rag_ans, gt_str)
            if rag_is_correct: correct_pure_rag += 1

            # 3. Adaptive Logic (ä¿å®ˆç­–ç•¥)
            # æˆ‘ä»¬éœ€è¦ä» Baseline çš„ 6 æ¬¡ç»“æœä¸­å–å‰ 3 æ¬¡æ¥æ¨¡æ‹Ÿ "Baseline (n=3)" ç”¨äºå¯¹æ¯”
            # ä½†è¿™é‡Œæˆ‘ä»¬ç›´æ¥ç”¨ Baseline (n=6) çš„ç»“æœä½œä¸ºåŸºç¡€ï¼Œå¦‚æœ RAG èƒ½æ‰“è´¥ n=6 çš„ Baselineï¼Œé‚£æ‰æ˜¯çœŸçš„å¼º
            
            # ä¸ºäº†å®ç° Adaptive é€»è¾‘ï¼Œæˆ‘ä»¬éœ€è¦ Baseline çš„ "ä¸ç¡®å®šæ€§"
            # å¦‚æœ Baseline (n=6) çš„ç¥¨æ•°å¾ˆåˆ†æ•£ (e.g. < 4/6)ï¼Œè¯´æ˜ Baseline ä¸è‡ªä¿¡
            
            final_ans = base_ans
            selection_source = "Baseline"

            # ç­–ç•¥ï¼š
            # æ¡ä»¶ A: RAG éå¸¸è‡ªä¿¡ (3/3) ä¸” ç­”æ¡ˆä¸ Baseline ä¸åŒ -> ç›¸ä¿¡ RAG (å¼ºä¿®æ­£)
            # æ¡ä»¶ B: Baseline ä¸è‡ªä¿¡ (<4/6) ä¸” RAG ç›¸å¯¹è‡ªä¿¡ (>=2/3) -> ç›¸ä¿¡ RAG (å¡«è¡¥ç©ºç™½)
            
            if rag_ans is not None and rag_ans != base_ans:
                if rag_count == 3:
                    final_ans = rag_ans
                    selection_source = "RAG (Strong)"
                elif base_count < 4 and rag_count >= 2:
                    final_ans = rag_ans
                    selection_source = "RAG (Fill Gap)"
            
            # æ£€æŸ¥ Adaptive ç»“æœ
            adaptive_is_correct = self.check_correct(final_ans, gt_str)
            if adaptive_is_correct: correct_adaptive += 1

            # æ‰“å°è¯¦ç»†æ—¥å¿—
            print(f"[{i+1}/{total}] Q: {questions[i][:50]}...")
            print(f"  Base(n=6): {base_ans} (Votes: {base_count}/{len(base_list)}) [{'âœ…' if base_is_correct else 'âŒ'}]")
            print(f"  RAG (n=3): {rag_ans} (Votes: {rag_count}/{len(rag_list)}) [{'âœ…' if rag_is_correct else 'âŒ'}]")
            print(f"  Strategy : {selection_source} -> Final: {final_ans} [{'âœ…' if adaptive_is_correct else 'âŒ'}]")
            print("-" * 40)

            # è®°å½•æœ‰è¶£çš„ Case (Baseline é”™ -> RAG å¯¹)
            if not base_is_correct and adaptive_is_correct:
                adaptive_log.append({
                    "q": questions[i],
                    "base": base_list,
                    "rag": rag_list,
                    "final": final_ans,
                    "gt": gt_str
                })

        # ======================================================================
        # Final Report
        # ======================================================================
        acc_base = correct_base / total * 100
        acc_pure_rag = correct_pure_rag / total * 100
        acc_adaptive = correct_adaptive / total * 100

        print("\n" + "="*60)
        print("ğŸ§ª Evaluation Results (Conservative/Adaptive Strategy)")
        print("="*60)
        print(f"1. Baseline (Majority Vote n=6): {acc_base:.2f}%")
        print(f"2. Pure RAG (Majority Vote n=3): {acc_pure_rag:.2f}%")
        print(f"3. Adaptive (Hybrid)           : {acc_adaptive:.2f}%  <-- Recommended")
        print("-" * 60)
        print(f"ğŸ“ˆ Improvement over Baseline: {acc_adaptive - acc_base:+.2f}%")
        print("="*60)
        
        # æ‰“å°å‡ ä¸ªä¿®æ­£æˆåŠŸçš„ä¾‹å­
        if adaptive_log:
            print("\nğŸŒŸ Examples where Adaptive RAG fixed Baseline:")
            for item in adaptive_log[:3]:
                print(f"Q: {item['q'][:100]}...")
                print(f"   Base Votes: {item['base']} -> Wrong")
                print(f"   RAG Votes : {item['rag']} -> Correct")
                print("-" * 30)

def load_config(path="configurations/evaluate.yaml"):
    with open(path, "r") as f:
        return yaml.safe_load(f)
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default="configurations/evaluate.yaml", help="YAML config path")
    args = parser.parse_args()
    config = load_config(args.config)
    evaluator = ScientificComparator(config)
    evaluator.run_scientific_test()
