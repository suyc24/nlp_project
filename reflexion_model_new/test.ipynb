{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/XXX/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ (vLLM Accelerated Mode)...\n",
      "INFO 12-20 23:32:45 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/root/autodl-tmp/hf_cache', 'max_model_len': 2048, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-7B-Instruct-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-20 23:32:50 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 12-20 23:32:50 [model.py:1661] Using max model len 2048\n",
      "INFO 12-20 23:32:51 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 12-20 23:32:51 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b1f37f3144470cb9d010c128a1a3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/XXX/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:08 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-7B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir='/root/autodl-tmp/hf_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:08 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.8:47933 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:08 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:09 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-7B-Instruct-AWQ...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:09 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.80it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.71it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:13 [default_loader.py:308] Loading weights took 1.23 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:14 [gpu_model_runner.py:3659] Model loading took 5.2036 GiB memory and 4.491841 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:20 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/135799e242/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:20 [backends.py:703] Dynamo bytecode transform time: 6.40 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:24 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.222 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:24 [monitor.py:34] torch.compile takes 7.62 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:25 [gpu_worker.py:375] Available KV cache memory: 8.74 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:25 [kv_cache_utils.py:1291] GPU KV cache size: 163,552 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:25 [kv_cache_utils.py:1296] Maximum concurrency for 2,048 tokens per request: 79.86x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:03<00:00, 15.53it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:31 [gpu_model_runner.py:4587] Graph capturing finished in 6 secs, took 0.50 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=92019)\u001b[0;0m INFO 12-20 23:33:31 [core.py:259] init engine (profile, create kv cache, warmup model) took 16.97 seconds\n",
      "INFO 12-20 23:33:33 [llm.py:360] Supported tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 23:33:33] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Loading Embedder (Force CPU to save GPU memory)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 23:33:40] INFO posthog.py:22: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Database reset at ./reflexion_full_db\n",
      "âš¡ï¸ æ­£åœ¨é¢„è®¡ç®— 200 æ¡é—®é¢˜çš„ Embedding (CPU Mode)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8473935c424f348aebf4e5638ad068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ å¼€å§‹å…¨é‡è¿›åŒ–è®­ç»ƒ (vLLM Speedup) - ç›®æ ‡å‡†ç¡®ç‡: 75.0%\n",
      "\n",
      "======== Epoch 1/10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ğŸ§  Abstracting 8 questions for better retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd169a211ed749e18362340f0c205a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/4 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ğŸ” Self-Exploring 8 samples (Batch Optimized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:22<01:06, 22.31s/it, Acc(ZS+RAG)=87.5%, DB=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ğŸ§  Abstracting 6 questions for better retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9e53b8dc2242f6befe543988653227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:31<01:06, 22.31s/it, Acc(ZS+RAG)=87.5%, DB=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ğŸ” Self-Exploring 6 samples (Batch Optimized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:42<00:42, 21.04s/it, Acc(ZS+RAG)=90.6%, DB=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ğŸ§  Abstracting 8 questions for better retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3803e3ada74ed08cba16571ef9c268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:52<00:42, 21.04s/it, Acc(ZS+RAG)=90.6%, DB=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ğŸ” Self-Exploring 8 samples (Batch Optimized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:05<00:21, 21.95s/it, Acc(ZS+RAG)=87.5%, DB=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ğŸ§  Abstracting 1 questions for better retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4e9e75f54a4882b045e6909ec8b6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:10<00:21, 21.95s/it, Acc(ZS+RAG)=87.5%, DB=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ğŸ” Self-Exploring 1 samples (Batch Optimized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:16<00:00, 19.03s/it, Acc(ZS+RAG)=87.5%, DB=0]\n",
      "[rank0]:[W1220 23:35:05.176816043 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 1 å®Œæˆ | ç»¼åˆå‡†ç¡®ç‡ (ZS+RAG): 88.50% (Target: 75.0%)\n",
      "ğŸ‰ æ­å–œï¼è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡ (88.50%)ï¼Œè®­ç»ƒå®Œæˆï¼\n",
      "ğŸ§¹ æ­£åœ¨æ¸…ç†æ˜¾å­˜å’Œ vLLM è¿›ç¨‹...\n",
      "âœ… æ¸…ç†å®Œæˆï¼\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from config import * \n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from vllm import LLM, SamplingParams\n",
    "from principle_manager import MemoryManager\n",
    "\n",
    "# ================= vLLM è¿›åŒ–è®­ç»ƒå™¨ (ä¸»è¦ä¿®æ”¹åŒºåŸŸ) =================\n",
    "class ReflexionTrainerFull:\n",
    "    def __init__(self):\n",
    "        print(\"ğŸš€ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ (vLLM Accelerated Mode)...\")\n",
    "        \n",
    "        self.llm = LLM(\n",
    "            model=MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "            tensor_parallel_size=1,\n",
    "            max_model_len=2048,\n",
    "            download_dir=HF_CACHE_DIR\n",
    "        )\n",
    "        \n",
    "        # æ ‡å‡†æ¨ç†å‚æ•°\n",
    "        self.params_inference = SamplingParams(\n",
    "            temperature=0.2, top_p=0.9, max_tokens=MAX_NEW_TOKENS,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        # åæ€å‚æ•° (é«˜åˆ›é€ æ€§)\n",
    "        self.params_reflection = SamplingParams(\n",
    "            temperature=0.7, top_p=0.9, max_tokens=MAX_NEW_TOKENS,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        # éªŒè¯å‚æ•° (ä½å®¹é”™)\n",
    "        self.params_verify = SamplingParams(\n",
    "            temperature=0.1, top_p=0.9, max_tokens=MAX_NEW_TOKENS,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        # æŠ½è±¡å‚æ•° (ä½é•¿åº¦)\n",
    "        self.params_abstract = SamplingParams(\n",
    "            temperature=0.5, top_p=0.9, max_tokens=64,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "\n",
    "        print(\"   -> Loading Embedder (Force CPU to save GPU memory)...\")\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device=\"cpu\", cache_folder=HF_CACHE_DIR)       \n",
    "        self.memory = MemoryManager(reset=True)\n",
    "        self.debug_log_path = \"debug_trace.jsonl\"\n",
    "        open(self.debug_log_path, \"w\").close() \n",
    "        \n",
    "    def log_debug(self, data):\n",
    "        import json\n",
    "        with open(self.debug_log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def batch_generate_vllm(self, prompts, sampling_params):\n",
    "        outputs = self.llm.generate(prompts, sampling_params, use_tqdm=False)\n",
    "        return [output.outputs[0].text.strip() for output in outputs]\n",
    "\n",
    "    def has_specific_numbers(self, text, question_text):\n",
    "        q_nums = set(re.findall(r'\\b\\d+\\b', question_text))\n",
    "        r_nums = set(re.findall(r'\\b\\d+\\b', text))\n",
    "        whitelist = {'1', '2', '3', '4', '10', '100', '180', '360'} \n",
    "        for n in r_nums:\n",
    "            if n in q_nums and n not in whitelist:\n",
    "                return True \n",
    "        return False\n",
    "\n",
    "    def clean_rule_text(self, text):\n",
    "        text = text.replace(\"```markdown\", \"\").replace(\"```\", \"\").strip()\n",
    "        patterns_to_remove = [\n",
    "            r\"^Sure,.*?\\n\", r\"^Here is.*?\\n\", r\"^Certainly.*?\\n\",\n",
    "            r\"^To summarize.*?\\n\", r\"You are an AI assistant.*?\",\n",
    "            r\"Assistant:.*?\", r\"Great job!.*?\", r\"#### \\d+\"\n",
    "        ]\n",
    "        for pat in patterns_to_remove:\n",
    "            text = re.sub(pat, \"\", text, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "        if text.count(\"**Trigger (A)**\") > 1:\n",
    "            first_occurrence = text.find(\"**Trigger (A)**\", text.find(\"**Trigger (A)**\") + 1)\n",
    "            text = text[:first_occurrence]\n",
    "        if text.count(\"**Strategy (B)**\") > 1:\n",
    "            first_occurrence = text.find(\"**Strategy (B)**\", text.find(\"**Strategy (B)**\") + 1)\n",
    "            text = text[:first_occurrence]\n",
    "        return text.strip()\n",
    "\n",
    "    def parse_reflection(self, text):\n",
    "        cleaned_text = self.clean_rule_text(text)\n",
    "        trigger_match = re.search(r\"(?:\\*\\*Trigger \\(A\\)\\*\\*|Trigger \\(A\\)|Trigger):?\\s*(.*?)(?=\\n|(?:\\*\\*Strategy)|$)\", cleaned_text, re.DOTALL | re.IGNORECASE)\n",
    "        strategy_match = re.search(r\"(?:\\*\\*Strategy \\(B\\)\\*\\*|Strategy \\(B\\)|Strategy):?\\s*(.*?)(?=\\n\\s*(?:\\*\\*Trigger)|$)\", cleaned_text, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if trigger_match and strategy_match:\n",
    "            t_text = trigger_match.group(1).strip()\n",
    "            s_text = strategy_match.group(1).strip()\n",
    "            t_text = re.sub(r\"\\s+\", \" \", t_text)\n",
    "            s_text = re.sub(r\"\\s+\", \" \", s_text)\n",
    "\n",
    "            if len(t_text) < 5 or len(s_text) < 5: return None\n",
    "            if len(t_text) > 300: t_text = t_text[:300] \n",
    "            if \"Strategy\" in t_text: t_text = t_text.split(\"Strategy\")[0].strip()\n",
    "            return t_text, s_text\n",
    "        return None\n",
    "\n",
    "    def construct_prompt(self, q, context=None):\n",
    "        if context:\n",
    "            content = f\"\"\"\n",
    "                [Reference Rules]\n",
    "                {context}\n",
    "\n",
    "                [Question]\n",
    "                {q}\n",
    "\n",
    "                [Instruction]\n",
    "                1. Read the Reference Rules carefully.\n",
    "                2. First, decide which rule is relevant to the question.\n",
    "                3. If a rule is relevant, write \"Selected Rule: [Rule Content]\".\n",
    "                4. If no rule is relevant, write \"No suitable rule found\".\n",
    "                5. Then, solve the problem step-by-step using the selected rule (if any).\n",
    "\n",
    "                Answer:\"\"\"\n",
    "        else:\n",
    "            content = f\"Question: {q}\\nAnswer step-by-step:\"\n",
    "            \n",
    "        return f\"<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    def verify_step_vllm(self, partial_prompt, gt, k=1):\n",
    "        \"\"\"\n",
    "        éªŒè¯å½“å‰æ­¥éª¤æ˜¯å¦æ­£ç¡®ã€‚\n",
    "        Prompt æ„é€ é€»è¾‘ï¼šç»™å®šå‰é¢çš„æ­¥éª¤ï¼Œè¯¢é—®æœ€åä¸€æ­¥æ˜¯å¦ä¸ Ground Truth çŸ›ç›¾ã€‚\n",
    "        \"\"\"\n",
    "        # æ„é€ éªŒè¯ Prompt\n",
    "        verify_content = f\"\"\"\n",
    "        I am solving a math problem.\n",
    "        [Previous Steps]\n",
    "        {partial_prompt.replace('<|im_start|>user', '').replace('<|im_start|>assistant', '').strip()}\n",
    "\n",
    "        [Ground Truth Answer]\n",
    "        {gt}\n",
    "\n",
    "        Evaluate ONLY the last step provided above.\n",
    "        Is this step logically correct and consistent with leading to the Ground Truth?\n",
    "        Answer strictly \"Yes\" or \"No\".\n",
    "        \"\"\"\n",
    "        full_prompt = f\"<|im_start|>user\\n{verify_content}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        \n",
    "        # è¿™é‡Œçš„ k å‚æ•°å¦‚æœæ˜¯ä¸ºäº†å¤šæ¬¡é‡‡æ ·éªŒè¯ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ‰©å±•ï¼Œç›®å‰ç®€åŒ–ä¸ºä¸€æ¬¡\n",
    "        outputs = self.batch_generate_vllm([full_prompt], self.params_verify)\n",
    "        resp = outputs[0].lower()\n",
    "        \n",
    "        return \"yes\" in resp and \"no\" not in resp\n",
    "\n",
    "    def self_explore_phase(self, epoch, still_incorrect_indices, chunk_questions, chunk_answers, index_to_rules=None):\n",
    "        \"\"\"\n",
    "        [æé€Ÿç‰ˆ] Self-Explore æœºåˆ¶ï¼š\n",
    "        1. Hindsight: å¼ºåˆ¶ç”Ÿæˆæ­£ç¡®è·¯å¾„ (Batch)\n",
    "        2. Contrast: å¯¹æ¯”ç”Ÿæˆè§„åˆ™ (Batch)\n",
    "        3. Verification: æ”¶é›†æ‰€æœ‰å€™é€‰è§„åˆ™ä¸€æ¬¡æ€§å¹¶è¡ŒéªŒè¯ (Batch)\n",
    "        \"\"\"\n",
    "        if not still_incorrect_indices:\n",
    "            return\n",
    "\n",
    "        tqdm.write(f\" ğŸ” Self-Exploring {len(still_incorrect_indices)} samples (Batch Optimized)...\")\n",
    "\n",
    "        # 1. å‡†å¤‡æ•°æ®\n",
    "        target_questions = [chunk_questions[i] for i in still_incorrect_indices]\n",
    "        target_answers = [chunk_answers[i] for i in still_incorrect_indices]\n",
    "\n",
    "        # ==========================================================\n",
    "        # Step 1: é”™è¯¯è·¯å¾„ & æ­£ç¡®è·¯å¾„\n",
    "        # ==========================================================\n",
    "        prompts_wrong = [self.construct_prompt(q) for q in target_questions]\n",
    "        traces_wrong = self.batch_generate_vllm(prompts_wrong, self.params_inference)\n",
    "\n",
    "        prompts_correct = []\n",
    "        for q, gt in zip(target_questions, target_answers):\n",
    "            hindsight_prompt = (\n",
    "                f\"Question: {q}\\n\"\n",
    "                f\"The correct answer is known to be: {gt}.\\n\"\n",
    "                f\"Please provide a correct, step-by-step mathematical derivation that results in this answer.\\n\"\n",
    "                f\"Answer step-by-step:\"\n",
    "            )\n",
    "            prompts_correct.append(f\"<|im_start|>user\\n{hindsight_prompt}<|im_end|>\\n<|im_start|>assistant\\n\")\n",
    "\n",
    "        traces_correct = self.batch_generate_vllm(prompts_correct, self.params_inference)\n",
    "\n",
    "        # ==========================================================\n",
    "        # Step 2: Contrast åæ€\n",
    "        # ==========================================================\n",
    "        contrast_prompts = []\n",
    "        contrast_metadata_map = []\n",
    "\n",
    "        for i in range(len(target_questions)):\n",
    "            q = target_questions[i]\n",
    "            gt = target_answers[i]\n",
    "            w_trace = traces_wrong[i]\n",
    "            c_trace = traces_correct[i]\n",
    "\n",
    "            if not self.check_answer(w_trace, gt) and self.check_answer(c_trace, gt):\n",
    "                \n",
    "                contrast_content = f\"\"\"You are a helpful math assistant. \n",
    "\n",
    "### Question\n",
    "{q}\n",
    "\n",
    "### Student's Reasoning\n",
    "{w_trace}\n",
    "\n",
    "### Right Answer\n",
    "{c_trace}\n",
    "\n",
    "### Instruction\n",
    "The student failed to solve the problem.\n",
    "Please provide a **generalized principle** to help the student understand the underlying concept and avoid similar mistakes.\n",
    "\n",
    "### Steps to Generate the Principle\n",
    "1. **Abstract the Problem**: Identify the general category of this problem (e.g., \"calculating compound probabilities\", \"finding the remaining amount after multiple subtractions\") without referring to specific numbers or items in this problem.\n",
    "2. **Identify the Method**: Determine the correct general method or logical step required to solve this type of problem.\n",
    "3. **Formulate the Hint**: Combine these into a single sentence.\n",
    "\n",
    "### Requirements\n",
    "- **Format**: Your output must be a single sentence following this pattern: \"To solve [Abstract Problem Description], consider [General Method/Principle].\"\n",
    "- **Strict Constraint**: Do NOT include any specific numbers (e.g., 16, 3) or specific nouns (e.g., eggs, ducks) from the current problem. The principle must be universal. \"\"\"\n",
    "\n",
    "                \n",
    "                \n",
    "#                 contrast_content = f\"\"\"You are a helpful math assistant. \n",
    "\n",
    "# ### Question\n",
    "# {q}\n",
    "\n",
    "# ### Student's Reasoning\n",
    "# {w_trace}\n",
    "\n",
    "# ### Right Answer\n",
    "# {c_trace}\n",
    "\n",
    "# ### Instruction\n",
    "# The student failed to solve the problem.\n",
    "# Please provide a **generalized principle** to help the student understand the underlying concept and avoid similar mistakes.\n",
    "\n",
    "# ### Steps to Generate the Principle\n",
    "# 1. **Abstract the Problem**: Identify the general category of this problem (e.g., \"calculating compound probabilities\", \"finding the remaining amount after multiple subtractions\") without referring to specific numbers or items in this problem.\n",
    "# 2. **Identify the Method**: Determine the correct general method or logical step required to solve this type of problem.\n",
    "# 3. **Formulate the Hint**: Combine these into a single sentence.\n",
    "\n",
    "# ### Requirements\n",
    "# - **Format**: Your output must be a single sentence following this pattern: \"To solve [Abstract Problem Description], consider [General Method/Principle].\"\n",
    "# - **Strict Constraint**: Do NOT include any specific numbers (e.g., 16, 3) or specific nouns (e.g., eggs, ducks) from the current problem. The principle must be universal. \"\"\"\n",
    "                contrast_prompts.append(f\"<|im_start|>user\\n{contrast_content}<|im_end|>\\n<|im_start|>assistant\\n\")\n",
    "                contrast_metadata_map.append(i)\n",
    "\n",
    "        if not contrast_prompts:\n",
    "            return\n",
    "\n",
    "        reflections = self.batch_generate_vllm(contrast_prompts, self.params_reflection)\n",
    "\n",
    "        # ==========================================================\n",
    "        # Step 3: è§£æè§„åˆ™ & æ„é€ éªŒè¯ promptï¼ˆğŸ”§ FIXï¼šå®Œæ•´ metadataï¼‰\n",
    "        # ==========================================================\n",
    "        verify_prompts = []\n",
    "        verify_candidates_metadata = []\n",
    "\n",
    "        for idx, reflection_text in enumerate(reflections):\n",
    "            original_idx = contrast_metadata_map[idx]\n",
    "\n",
    "            q = target_questions[original_idx]\n",
    "            gt = target_answers[original_idx]\n",
    "\n",
    "            parsed = self.parse_reflection(reflection_text)\n",
    "            if not parsed:\n",
    "                continue\n",
    "\n",
    "            trigger, strategy = parsed\n",
    "            if self.has_specific_numbers(strategy, q):\n",
    "                continue\n",
    "            if len(strategy) < 10:\n",
    "                continue\n",
    "\n",
    "            temp_embed = self.embedder.encode(trigger + \" \" + strategy, convert_to_numpy=True)\n",
    "            try:\n",
    "                existing = self.memory.collection.query(query_embeddings=[temp_embed], n_results=1)\n",
    "                if existing['ids'] and existing['ids'][0]:\n",
    "                    # å¦‚æœç›¸ä¼¼åº¦éå¸¸é«˜ (distance < 0.2)ï¼Œè¯´æ˜åº“é‡Œå·²ç»æœ‰äº†ï¼Œç›´æ¥è·³è¿‡ï¼Œçœä¸‹éªŒè¯çš„æ—¶é—´\n",
    "                    if existing['distances'][0][0] < 0.2:\n",
    "                        print(f\"   Duplicate rule detected (dist={existing['distances'][0][0]:.3f}), skipping verification.\")\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "            v_prompt = self.construct_prompt(q, context=strategy)\n",
    "            verify_prompts.append(v_prompt)\n",
    "\n",
    "            # ğŸ”§ FIXï¼šä¿å­˜ question / index / gtï¼Œé¿å…é”™ä½\n",
    "            verify_candidates_metadata.append({\n",
    "                \"trigger\": trigger,\n",
    "                \"strategy\": strategy,\n",
    "                \"question\": q,\n",
    "                \"gt\": gt,\n",
    "                \"local_idx\": original_idx\n",
    "            })\n",
    "\n",
    "        # ==========================================================\n",
    "        # Step 4: Batch éªŒè¯\n",
    "        # ==========================================================\n",
    "        if not verify_prompts:\n",
    "            return\n",
    "\n",
    "        verify_outputs = self.batch_generate_vllm(verify_prompts, self.params_verify)\n",
    "\n",
    "        new_patterns = []\n",
    "        new_strategies = []\n",
    "        new_embeddings_inputs = []\n",
    "\n",
    "        for i, pred in enumerate(verify_outputs):\n",
    "            meta = verify_candidates_metadata[i]\n",
    "            gt = meta[\"gt\"]\n",
    "            q = meta[\"question\"]\n",
    "\n",
    "            if self.check_answer(pred, gt):\n",
    "                self.log_debug({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"phase\": \"rule_verified\",\n",
    "                    \"trigger\": meta[\"trigger\"],\n",
    "                    \"strategy\": meta[\"strategy\"],\n",
    "                    \"question\": q,   # âœ… ç»å¯¹æ­£ç¡®\n",
    "                    \"gt\": gt\n",
    "                })\n",
    "\n",
    "                tqdm.write(f\"    âœ… Rule Verified: {meta['trigger'][:40]}... -> {meta['strategy'][:40]}...\")\n",
    "                new_patterns.append(meta[\"trigger\"])\n",
    "                new_strategies.append(meta[\"strategy\"])\n",
    "                new_embeddings_inputs.append(meta[\"trigger\"])\n",
    "\n",
    "        if new_patterns:\n",
    "            print(f\" ğŸ’¾ Adding {len(new_patterns)} high-quality rules to memory...\")\n",
    "            embeddings = self.embedder.encode(new_embeddings_inputs, convert_to_numpy=True)\n",
    "            self.memory.add_experience_batch(new_patterns, new_strategies, embeddings)\n",
    "\n",
    "\n",
    "    def run_full_evolution(self):\n",
    "        # 1. å‡†å¤‡æ•°æ®\n",
    "        dataset = load_dataset(\"gsm8k\", \"main\")['train'].select(range(200)) \n",
    "        \n",
    "        total_len = len(dataset)\n",
    "        print(f\"âš¡ï¸ æ­£åœ¨é¢„è®¡ç®— {total_len} æ¡é—®é¢˜çš„ Embedding (CPU Mode)...\")\n",
    "        \n",
    "        all_questions_raw = dataset['question']\n",
    "        all_answers_raw = dataset['answer']\n",
    "        \n",
    "        # é¢„è®¡ç®— Embeddings (Epoch é—´å¤ç”¨)\n",
    "        all_q_embeddings = self.embedder.encode(all_questions_raw, batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
    "        \n",
    "        print(f\"ğŸ”¥ å¼€å§‹å…¨é‡è¿›åŒ–è®­ç»ƒ (vLLM Speedup) - ç›®æ ‡å‡†ç¡®ç‡: {TARGET_ACCURACY}%\")\n",
    "\n",
    "        epoch = 0\n",
    "        best_acc = 0.0\n",
    "\n",
    "        while best_acc < TARGET_ACCURACY and epoch < MAX_EPOCHS:\n",
    "            epoch += 1\n",
    "            print(f\"\\n======== Epoch {epoch}/{MAX_EPOCHS} ========\")\n",
    "            \n",
    "            # 1. Shuffle æ•°æ®ç´¢å¼•\n",
    "            indices = np.random.permutation(total_len)\n",
    "            \n",
    "            epoch_correct_count = 0\n",
    "            epoch_total_count = 0\n",
    "            \n",
    "            pbar = tqdm(range(0, total_len, CHUNK_SIZE), desc=f\"Epoch {epoch} Training\")\n",
    "            \n",
    "            for chunk_start in pbar:\n",
    "                chunk_end = min(chunk_start + CHUNK_SIZE, total_len)\n",
    "                current_batch_indices = indices[chunk_start:chunk_end]\n",
    "                \n",
    "                # è·å–å½“å‰ Batch çš„åŸå§‹æ•°æ®\n",
    "                chunk_questions = [all_questions_raw[i] for i in current_batch_indices]\n",
    "                chunk_answers = [all_answers_raw[i] for i in current_batch_indices]\n",
    "                chunk_q_embeddings = all_q_embeddings[current_batch_indices]\n",
    "                \n",
    "                # ================= é˜¶æ®µ 1: Zero-shot æ¨ç† =================\n",
    "                zero_shot_prompts = [self.construct_prompt(q) for q in chunk_questions]\n",
    "                zs_outputs = self.batch_generate_vllm(zero_shot_prompts, self.params_inference)\n",
    "                \n",
    "                # è¯„ä¼° Zero-shot ç»“æœ\n",
    "                zs_is_correct = []\n",
    "                incorrect_local_indices = [] # è®°å½•åœ¨è¿™ä¸ª chunk ä¸­åšé”™çš„ä¸‹æ ‡\n",
    "                \n",
    "                for idx, pred in enumerate(zs_outputs):\n",
    "                    gt = chunk_answers[idx]\n",
    "                    is_right = self.check_answer(pred, gt)\n",
    "                    zs_is_correct.append(is_right)\n",
    "\n",
    "                    if not is_right:\n",
    "                        incorrect_local_indices.append(idx)\n",
    "                    epoch_total_count += 1\n",
    "\n",
    "                chunk_final_correct = zs_is_correct[:]\n",
    "                \n",
    "                # ================= é˜¶æ®µ 2: RAG é‡ç®— (ä»…é’ˆå¯¹é”™é¢˜) =================\n",
    "                rag_usage_for_update = []\n",
    "                rag_is_correct_for_update = []\n",
    "                rag_outputs_for_update = []              \n",
    "                still_incorrect_indices = []   # RAG åä¾ç„¶åšé”™çš„ä¸‹æ ‡ï¼Œç”¨äº Self-Explore\n",
    "                \n",
    "                # ç”¨äºè®°å½•æ¯é“é”™é¢˜ç”¨äº†ä»€ä¹ˆè§„åˆ™ï¼Œä¼ ç»™ Self-Explore æ‰“å°æ—¥å¿—\n",
    "                rag_rules_map = {} \n",
    "\n",
    "                if incorrect_local_indices:\n",
    "                    # 1. å‡†å¤‡é”™é¢˜æ•°æ®\n",
    "                    wrong_questions = [chunk_questions[i] for i in incorrect_local_indices]\n",
    "                    wrong_embeddings = chunk_q_embeddings[incorrect_local_indices]\n",
    "                    wrong_answers = [chunk_answers[i] for i in incorrect_local_indices]\n",
    "                    \n",
    "                    # 2. æ£€ç´¢è§„åˆ™\n",
    "                    print(f\"    ğŸ§  Abstracting {len(wrong_questions)} questions for better retrieval...\")\n",
    "                    abstract_intent_list = self.batch_abstract_for_retrieval(wrong_questions)\n",
    "\n",
    "                    # 2. å¯¹â€œæŠ½è±¡æè¿°â€è¿›è¡Œ Embeddingï¼Œè€Œä¸æ˜¯å¯¹åŸé¢˜ Embedding\n",
    "                    query_embeddings = self.embedder.encode(abstract_intent_list, convert_to_numpy=True).tolist()\n",
    "                    \n",
    "                    # 3. ä½¿ç”¨æŠ½è±¡ Embedding è¿›è¡Œæ£€ç´¢\n",
    "                    retrieved_batch = self.memory.batch_retrieve(query_embeddings, top_k=3, threshold=0.6)\n",
    "                    \n",
    "                    rag_prompts = []\n",
    "                    \n",
    "                    for k, q in enumerate(wrong_questions):\n",
    "                        original_idx = incorrect_local_indices[k]\n",
    "                        rules_list = retrieved_batch[k]\n",
    "                        \n",
    "                        if rules_list:\n",
    "                            context_text = \"\\n\".join([f\"[Rule {i+1}]: {r[0]}\" for i, r in enumerate(rules_list)])\n",
    "                            rag_prompts.append(self.construct_prompt(q, context_text))\n",
    "                            rag_usage_for_update.append(rules_list)\n",
    "                            rag_rules_map[original_idx] = [r[0] for r in rules_list]\n",
    "                        else:\n",
    "                            # å¦‚æœæ²¡æ£€ç´¢åˆ°è§„åˆ™ï¼Œå°±æ²¡å¿…è¦ RAG é‡ç®—äº†\n",
    "                            rag_usage_for_update.append([]) \n",
    "                            rag_prompts.append(None) \n",
    "                            still_incorrect_indices.append(original_idx)\n",
    "\n",
    "                    # 3. æ‰§è¡Œ RAG æ¨ç† (åªæ¨ç†æœ‰ Prompts çš„éƒ¨åˆ†)\n",
    "                    real_rag_prompts = [p for p in rag_prompts if p is not None]\n",
    "                    if real_rag_prompts:\n",
    "                        real_rag_outputs = self.batch_generate_vllm(real_rag_prompts, self.params_inference)\n",
    "                    \n",
    "                    # 4. è¯„ä¼° RAG ç»“æœå¹¶å‡†å¤‡ Update æ•°æ®\n",
    "                    output_cursor = 0\n",
    "\n",
    "                    for k, q in enumerate(wrong_questions):\n",
    "                        original_idx = incorrect_local_indices[k]\n",
    "                        \n",
    "                        if rag_prompts[k] is None:\n",
    "                            continue\n",
    "                            \n",
    "                        pred = real_rag_outputs[output_cursor]\n",
    "                        output_cursor += 1\n",
    "                        \n",
    "                        gt = wrong_answers[k]\n",
    "                        is_right = self.check_answer(pred, gt)\n",
    "                        \n",
    "                        # è®°å½• Update æ•°æ®\n",
    "                        rag_is_correct_for_update.append(is_right)\n",
    "                        rag_outputs_for_update.append(pred)\n",
    "                        \n",
    "                        self.log_debug({\n",
    "                            \"epoch\": epoch,\n",
    "                            \"phase\": \"rag\",\n",
    "                            \"question\": q,\n",
    "                            \"used_rules\": rag_rules_map.get(original_idx, []),\n",
    "                            \"pred\": pred,\n",
    "                            \"gt\": gt,\n",
    "                            \"is_correct\": is_right\n",
    "                        })\n",
    "\n",
    "                        if is_right:\n",
    "                            chunk_final_correct[original_idx] = True\n",
    "                        else:\n",
    "                            still_incorrect_indices.append(original_idx)\n",
    "                    \n",
    "                    # 5. æ›´æ–° Memory åˆ†æ•°\n",
    "                    final_usage = [rag_usage_for_update[k] for k in range(len(wrong_questions)) if rag_prompts[k] is not None]\n",
    "                    \n",
    "                    if final_usage:\n",
    "                        self.memory.update_scores_batch(final_usage, rag_is_correct_for_update, rag_outputs_for_update)\n",
    "\n",
    "                # ================= é˜¶æ®µ 3: Self-Explore (ä»…é’ˆå¯¹ RAG åä¾ç„¶é”™è¯¯çš„é¢˜) =================\n",
    "                if still_incorrect_indices:\n",
    "                    self.self_explore_phase(\n",
    "                        epoch,\n",
    "                        still_incorrect_indices, \n",
    "                        chunk_questions, \n",
    "                        chunk_answers, \n",
    "                        index_to_rules=rag_rules_map\n",
    "                    )\n",
    "\n",
    "                # --- å®šæœŸæ·˜æ±° ---\n",
    "                if (chunk_start // CHUNK_SIZE) % 5 == 0:\n",
    "                    self.memory.prune_db(threshold=0.25)\n",
    "\n",
    "                epoch_correct_count += sum(chunk_final_correct)\n",
    "\n",
    "                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯ (ZS+RAG Accuracy)\n",
    "                batch_acc = sum(chunk_final_correct) / len(chunk_questions) * 100\n",
    "                pbar.set_postfix({\"Acc(ZS+RAG)\": f\"{batch_acc:.1f}%\", \"DB\": self.memory.collection.count()})\n",
    "            \n",
    "            # --- Epoch æ€»ç»“ ---\n",
    "            current_epoch_acc = (epoch_correct_count / epoch_total_count) * 100\n",
    "            print(f\"\\nğŸ“Š Epoch {epoch} å®Œæˆ | ç»¼åˆå‡†ç¡®ç‡ (ZS+RAG): {current_epoch_acc:.2f}% (Target: {TARGET_ACCURACY}%)\")\n",
    "            \n",
    "            if current_epoch_acc > best_acc:\n",
    "                best_acc = current_epoch_acc\n",
    "            \n",
    "            if current_epoch_acc >= TARGET_ACCURACY:\n",
    "                print(f\"ğŸ‰ æ­å–œï¼è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡ ({current_epoch_acc:.2f}%)ï¼Œè®­ç»ƒå®Œæˆï¼\")\n",
    "                break\n",
    "            elif epoch >= MAX_EPOCHS:\n",
    "                print(\"âš ï¸ è¾¾åˆ°æœ€å¤§ Epoch é™åˆ¶ï¼Œåœæ­¢è®­ç»ƒã€‚\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"ğŸ”„ è¡¨ç°æœªè¾¾æ ‡ï¼Œç»§ç»­ä¸‹ä¸€è½®è®­ç»ƒ...\")\n",
    "\n",
    "    def batch_abstract_for_retrieval(self, questions):\n",
    "        \"\"\"\n",
    "        å°†å…·ä½“é—®é¢˜è½¬åŒ–ä¸ºæŠ½è±¡çš„æ•°å­¦è€ƒç‚¹æè¿°ï¼Œç”¨äºæ£€ç´¢ã€‚\n",
    "        \"\"\"\n",
    "        prompts = []\n",
    "        for q in questions:\n",
    "            content = f\"\"\"\n",
    "            Task: Identify the core mathematical concept and intent of the following problem.\n",
    "            Output a concise, abstract description (1 sentence).\n",
    "            Do NOT include specific numbers or proper names.\n",
    "\n",
    "            [Example]\n",
    "            Q: John has 5 apples and buys 3 more. How many?\n",
    "            A: Calculating the total sum of objects using addition.\n",
    "\n",
    "            [Target]\n",
    "            Q: {q}\n",
    "            A:\"\"\"\n",
    "            prompts.append(f\"<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\")\n",
    "        \n",
    "        abstract_queries = self.batch_generate_vllm(prompts, self.params_abstract)\n",
    "        \n",
    "        return abstract_queries\n",
    "\n",
    "    def extract_number(self, text):\n",
    "        if not text: return None\n",
    "        text = text.replace(',', '')\n",
    "        matches = re.findall(r'-?\\d+\\.?\\d*', text)\n",
    "        if matches: return float(matches[-1])\n",
    "        return None\n",
    "\n",
    "    def check_answer(self, pred, gt):\n",
    "        if \"####\" in gt:\n",
    "            gold = self.extract_number(gt.split(\"####\")[1])\n",
    "        else:\n",
    "            gold = self.extract_number(gt)\n",
    "        pred_num = self.extract_number(pred)\n",
    "        if gold is None or pred_num is None: return False\n",
    "        return abs(gold - pred_num) < 1e-4\n",
    "\n",
    "    def cleanup(self):\n",
    "        print(\"ğŸ§¹ æ­£åœ¨æ¸…ç†æ˜¾å­˜å’Œ vLLM è¿›ç¨‹...\")\n",
    "        if hasattr(self, 'llm'):\n",
    "            del self.llm\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            import ray\n",
    "            if ray.is_initialized():\n",
    "                ray.shutdown()\n",
    "        except ImportError:\n",
    "            pass\n",
    "        print(\"âœ… æ¸…ç†å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = None\n",
    "    try:\n",
    "        trainer = ReflexionTrainerFull()\n",
    "        trainer.run_full_evolution()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ›‘ ç”¨æˆ·å¼ºåˆ¶åœæ­¢è®­ç»ƒ\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        if trainer is not None:\n",
    "            trainer.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XXX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
