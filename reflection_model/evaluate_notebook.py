import os
import torch
import re
import json
import shutil
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import chromadb
from peft import PeftModel

# ================= é…ç½® =================
BASE_MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
LORA_PATH = "./evolved_qwen_lora" # ä½ çš„ LoRA æƒé‡è·¯å¾„
DB_PATH = "./reflexion_full_db"   # ä½ çš„ç»éªŒåº“è·¯å¾„
BATCH_SIZE = 32
OUTPUT_FILE = "final_evaluation_report.json"

# ================= å·¥å…·ç±» =================
class MemoryManager:
    def __init__(self):
        self.client = chromadb.PersistentClient(path=DB_PATH)
        self.collection = self.client.get_collection(name="rule_book")
        self.stats = {} 
        self._load_cache()

    def _load_cache(self):
        try:
            existing = self.collection.get()
            if existing['ids']:
                for i, sid in enumerate(existing['ids']):
                    self.stats[sid] = existing['metadatas'][i]
        except:
            print("âš ï¸ è­¦å‘Šï¼šæ— æ³•åŠ è½½ç»éªŒåº“ç¼“å­˜ï¼Œå¯èƒ½æ˜¯æ–°åº“æˆ–è·¯å¾„é”™è¯¯")

    def batch_retrieve(self, query_embeddings):
        count = self.collection.count()
        if count == 0: return [None] * len(query_embeddings)
        
        results_list = []
        try:
            results = self.collection.query(query_embeddings=query_embeddings, n_results=min(5, count))
            for i in range(len(query_embeddings)):
                if results['ids'][i]:
                    # å– Top-1
                    content = results['documents'][i][0]
                    dist = results['distances'][i][0]
                    sid = results['ids'][i][0]
                    meta = self.stats.get(sid, results['metadatas'][i][0])
                    # è¿”å›å†…å®¹å’Œè·ç¦»
                    results_list.append((content, dist))
                else:
                    results_list.append(None)
        except:
            return [None] * len(query_embeddings)
            
        return results_list

# ================= è¯„ä¼°å™¨ =================
class Evaluator:
    def __init__(self):
        print(f"ğŸš€ 1. åŠ è½½åŸºåº§æ¨¡å‹ (Base Model): {BASE_MODEL_NAME}...")
        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, padding_side="left")
        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # å…ˆåªåŠ è½½åŸºåº§æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_NAME, 
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32, 
            device_map="auto"
        )
        
        print("ğŸ“¥ åŠ è½½ Embedder...")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)
        self.memory = MemoryManager()
        
    def load_lora(self):
        """åœ¨åŸºåº§æ¨¡å‹ä¸ŠæŒ‚è½½ LoRA"""
        print(f"\nğŸ§¬ 2. æŒ‚è½½è¿›åŒ–æƒé‡ (LoRA): {LORA_PATH}...")
        # ä½¿ç”¨ PeftModel åŠ è½½ LoRAï¼Œä¸è¿›è¡Œ merge_and_unload ä»¥ä¾¿å¯¹æ¯”ï¼ˆæˆ–è€…ç›´æ¥è¦†ç›–ï¼‰
        self.model = PeftModel.from_pretrained(self.model, LORA_PATH)
        self.model.eval()

    def batch_generate(self, prompts):
        inputs = self.tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs, 
                max_new_tokens=512,
                temperature=0.01, # æµ‹è¯•æ—¶è¶‹è¿‘äº 0ï¼Œæ¶ˆé™¤éšæœºæ€§
                do_sample=False,  # ä½¿ç”¨ Greedy Search ä¿è¯ç»“æœç¨³å®š
                pad_token_id=self.tokenizer.pad_token_id
            )
        decoded = self.tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)
        return [d.strip() for d in decoded]

    def extract_answer(self, text):
        if not text: return None
        text = text.replace(',', '')
        matches = re.findall(r'-?\d+\.?\d*', text)
        if matches: return float(matches[-1])
        return None

    def check_correct(self, pred_str, ground_truth):
        if "####" in ground_truth:
            gold = self.extract_answer(ground_truth.split("####")[1])
        else:
            gold = self.extract_answer(ground_truth)
        pred = self.extract_answer(pred_str)
        if gold is None or pred is None: return False
        return abs(gold - pred) < 1e-4

    def run_full_comparison(self):
        dataset = load_dataset("gsm8k", "main")['test']
        print(f"\n=== å¼€å§‹ä¸‰æ–¹å¯¹æ¯”æµ‹è¯• (Test Set: {len(dataset)}) ===")
        
        total = len(dataset)
        
        # ç»“æœå®¹å™¨
        results_base = []
        results_lora_naked = []
        results_lora_rag = []
        
        questions = dataset['question']
        ground_truths = dataset['answer']
        
        # --- ç¬¬ä¸€é˜¶æ®µï¼šæµ‹è¯•çº¯åŸºåº§æ¨¡å‹ (Base Model) ---
        print("\n[Phase 1] æµ‹è¯•åŸºåº§æ¨¡å‹ (Base Model)...")
        for i in tqdm(range(0, total, BATCH_SIZE)):
            batch_q = questions[i : i+BATCH_SIZE]
            prompts = [f"Question: {q}\nLet's think step by step.\nAnswer:" for q in batch_q]
            answers = self.batch_generate(prompts)
            results_base.extend(answers)
            
        # --- ç¬¬äºŒé˜¶æ®µï¼šåŠ è½½ LoRA å¹¶æµ‹è¯• ---
        self.load_lora()
        
        # é¢„å…ˆæ£€ç´¢æ‰€æœ‰ RAG å†…å®¹ (ä¸ºäº†æ•ˆç‡)
        print("\n[Retrieval] æ­£åœ¨é¢„æ£€ç´¢ç»éªŒåº“...")
        all_rag_contexts = []
        for i in tqdm(range(0, total, BATCH_SIZE)):
            batch_q = questions[i : i+BATCH_SIZE]
            q_embeds = self.embedder.encode(batch_q).tolist()
            # æ£€ç´¢
            retrieved = self.memory.batch_retrieve(q_embeds)
            all_rag_contexts.extend(retrieved)

        print("\n[Phase 2 & 3] æµ‹è¯•è¿›åŒ–æ¨¡å‹ (LoRA & RAG)...")
        for i in tqdm(range(0, total, BATCH_SIZE)):
            batch_q = questions[i : i+BATCH_SIZE]
            
            # A. LoRA Naked Prompts
            prompts_naked = [f"Question: {q}\nLet's think step by step.\nAnswer:" for q in batch_q]
            
            # B. LoRA RAG Prompts
            prompts_rag = []
            for j, q in enumerate(batch_q):
                res = all_rag_contexts[i+j]
                if res:
                    content, dist = res
                    # å¦‚æœè·ç¦»å¤ªè¿œï¼Œå…¶å®ä¸åº”è¯¥ç”¨ï¼Œè¿™é‡Œä¸ºäº†å¼ºåˆ¶æµ‹è¯•RAGæ•ˆæœï¼Œåªè¦æœ‰å°±ç”¨
                    p = f"Hint: {content}\nQuestion: {q}\nAnswer step-by-step:"
                else:
                    p = f"Question: {q}\nLet's think step by step.\nAnswer:"
                prompts_rag.append(p)
            
            # æ¨ç†
            ans_naked = self.batch_generate(prompts_naked)
            ans_rag = self.batch_generate(prompts_rag)
            
            results_lora_naked.extend(ans_naked)
            results_lora_rag.extend(ans_rag)

        # --- ç»Ÿè®¡åˆ†æ•° ---
        correct_base = 0
        correct_lora = 0
        correct_rag = 0
        
        for i in range(total):
            gt = ground_truths[i]
            if self.check_correct(results_base[i], gt): correct_base += 1
            if self.check_correct(results_lora_naked[i], gt): correct_lora += 1
            if self.check_correct(results_lora_rag[i], gt): correct_rag += 1
            
        acc_base = correct_base / total * 100
        acc_lora = correct_lora / total * 100
        acc_rag = correct_rag / total * 100
        
        print("\n" + "="*50)
        print("ğŸ“Š æœ€ç»ˆä¸‰æ–¹å¯¹æ¯”æŠ¥å‘Š")
        print("="*50)
        print(f"1. Base Model (0.5B åŸç”Ÿ): {acc_base:.2f}%")
        print(f"2. LoRA Only (å†…åŒ–èƒ½åŠ›)   : {acc_lora:.2f}%")
        print(f"3. LoRA + RAG (å®Œæ•´èƒ½åŠ›)  : {acc_rag:.2f}%")
        print("-" * 50)
        print(f"è®­ç»ƒå¸¦æ¥çš„å†…åŒ–æå‡: {acc_lora - acc_base:+.2f}%")
        print(f"RAGå¸¦æ¥çš„é¢å¤–æå‡ : {acc_rag - acc_lora:+.2f}%")
        print(f"æ€»æå‡              : {acc_rag - acc_base:+.2f}%")
        print("="*50)

if __name__ == "__main__":
    evaluator = Evaluator()
    evaluator.run_full_comparison()