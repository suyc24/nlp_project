import os
import shutil
import time
import random
import re
import torch
import chromadb
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
DB_PATH = "./reflexion_full_db"
CHUNK_SIZE = 64         
INFERENCE_BATCH_SIZE = 32 
TRAIN_BATCH_SIZE = 8    
LEARNING_RATE = 2e-5    
MAX_NEW_TOKENS = 256

# ================= 1. è®°å¿†ç®¡ç†å™¨ (ä¿®å¤ç‰ˆ) =================
class MemoryManager:
    def __init__(self, reset=True):
        if reset and os.path.exists(DB_PATH):
            shutil.rmtree(DB_PATH)
        self.client = chromadb.PersistentClient(path=DB_PATH)
        self.collection = self.client.get_or_create_collection(name="rule_book")
        
        # å†…å­˜ç¼“å­˜ç»Ÿè®¡æ•°æ®
        self.skill_stats = {} 

    def batch_retrieve(self, query_embeddings, n_results=1):
        """æ‰¹é‡æ£€ç´¢"""
        count = self.collection.count()
        if count == 0: return [None] * len(query_embeddings)
        
        results_list = []
        try:
            results = self.collection.query(query_embeddings=query_embeddings, n_results=n_results)
            for i in range(len(query_embeddings)):
                if results['documents'][i]:
                    # è¿”å› (Content, Distance, ID)
                    doc = results['documents'][i][0]
                    dist = results['distances'][i][0]
                    sid = results['ids'][i][0]
                    results_list.append((doc, dist, sid))
                else:
                    results_list.append(None)
        except:
            return [None] * len(query_embeddings)
            
        return results_list

    def add_experience_batch(self, patterns_A, strategies_B, embeddings_A):
        """æ‰¹é‡å†™å…¥"""
        if not patterns_A: return
        # ç”Ÿæˆå”¯ä¸€ID
        new_ids = [f"rule_{int(time.time())}_{i}_{random.randint(0,999)}" for i in range(len(patterns_A))]
        metadatas = [{"pattern": p} for p in patterns_A]
        
        self.collection.add(
            ids=new_ids,
            embeddings=embeddings_A,
            documents=strategies_B,
            metadatas=metadatas
        )
        # åˆå§‹åŒ–åˆ†æ•°
        for sid in new_ids:
            self.skill_stats[sid] = {"score": 0.5, "usage": 0, "history_correct": 0}

    def update_scores_batch(self, usage_data, is_correct_list, model_outputs):
        """
        ã€ä¿®å¤ã€‘æ›´æ–°åˆ†æ•°é€»è¾‘
        :param usage_data: List of (skill_id, skill_content) or None
        :param is_correct_list: List of Boolean
        :param model_outputs: List of String (æ¨¡å‹çš„å›ç­”)
        """
        for i, item in enumerate(usage_data):
            if item is None: continue # æ²¡ç”¨RAG
            
            sid, content = item
            is_correct = is_correct_list[i]
            output_text = model_outputs[i]
            
            # åˆå§‹åŒ– stats
            if sid not in self.skill_stats:
                self.skill_stats[sid] = {"score": 0.5, "usage": 0, "history_correct": 0}
            stats = self.skill_stats[sid]
            
            # --- ç­–ç•¥ï¼šæ— è¾œæ—è§‚è€…ä¿æŠ¤ ---
            # å¦‚æœç»éªŒçš„å†…å®¹ï¼ˆTrigger/Strategyï¼‰å®Œå…¨æ²¡æœ‰å‡ºç°åœ¨æ¨¡å‹çš„æ€è€ƒä¸­ï¼Œè¯´æ˜æ¨¡å‹å¯èƒ½å¿½ç•¥äº†å®ƒ
            # è¿™ç§æƒ…å†µä¸‹ï¼Œä¸åº”è¯¥å› ä¸ºåšé”™äº†è€Œæƒ©ç½šç»éªŒ
            # (ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼Œå–å‰20ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹)
            if len(content) > 10:
                fingerprint = content[:10]
                if fingerprint not in output_text and not is_correct:
                    continue # æ²¡ç”¨ä¸Šï¼Œä¸”åšé”™äº† -> ä¸æ€ªç»éªŒï¼Œè·³è¿‡æ›´æ–°

            stats['usage'] += 1
            
            if is_correct:
                stats['history_correct'] += 1
                # å¥–åŠ±
                stats['score'] = min(1.0, stats['score'] + 0.1)
            else:
                # æƒ©ç½š
                penalty = 0.2
                # è€å…µå…ç–«ï¼šå¦‚æœå†å²æˆ˜ç»©å¥½ï¼Œæƒ©ç½šå‡è½»
                if stats['history_correct'] > 10: penalty = 0.1
                if stats['history_correct'] > 50: penalty = 0.05
                
                stats['score'] = max(0.0, stats['score'] - penalty)

    def prune_db(self, min_usage=5, threshold=0.2):
        """æ·˜æ±°é€»è¾‘"""
        ids_to_delete = []
        for sid, stats in list(self.skill_stats.items()):
            if stats['usage'] >= min_usage and stats['score'] < threshold:
                ids_to_delete.append(sid)
        
        if ids_to_delete:
            self.collection.delete(ids=ids_to_delete)
            for sid in ids_to_delete:
                del self.skill_stats[sid]
            return len(ids_to_delete)
        return 0

# ================= 2. å…¨é‡è¿›åŒ–è®­ç»ƒå™¨ (ä¿®å¤è°ƒç”¨) =================
class ReflexionTrainerFull:
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ (Full Set Mode)...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side="left")
        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME, torch_dtype=torch.float16, device_map="auto"
        )
        
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM, inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        )
        self.model = get_peft_model(self.base_model, peft_config)
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=LEARNING_RATE)
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device="cuda" if torch.cuda.is_available() else "cpu")
        self.memory = MemoryManager(reset=True)

    def batch_generate(self, prompts, temperature=0.5):
        self.model.eval()
        inputs = self.tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs, max_new_tokens=MAX_NEW_TOKENS, temperature=temperature, do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )
        decoded = self.tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)
        return [d.strip() for d in decoded]

    def train_on_chunk(self, training_data):
        if not training_data: return 0.0
        self.model.train()
        total_loss = 0; steps = 0
        self.tokenizer.padding_side = "right"
        
        for i in range(0, len(training_data), TRAIN_BATCH_SIZE):
            batch = training_data[i : i + TRAIN_BATCH_SIZE]
            texts = [f"Question: {q}\nAnswer: {a}{self.tokenizer.eos_token}" for q, a in batch]
            inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(self.model.device)
            outputs = self.model(**inputs, labels=inputs.input_ids)
            loss = outputs.loss
            loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()
            total_loss += loss.item(); steps += 1
            
        self.tokenizer.padding_side = "left"
        return total_loss / max(1, steps)

    def parse_reflection(self, texts):
        patterns = []; strategies = []
        for text in texts:
            try:
                if "**Trigger (A)**:" in text and "**Strategy (B)**:" in text:
                    parts = text.split("**Strategy (B)**:")
                    p = parts[0].replace("**Trigger (A)**:", "").strip()
                    s = parts[1].strip()
                    if len(p) > 5 and len(s) > 5:
                        patterns.append(p); strategies.append(s)
            except: continue
        return patterns, strategies

    def run_full_evolution(self):
        dataset = load_dataset("gsm8k", "main")['train']
        total_len = len(dataset)
        print(f"ğŸ”¥ å¼€å§‹å…¨é‡è¿›åŒ–è®­ç»ƒï¼Œæ•°æ®æ€»é‡: {total_len}")

        for chunk_start in range(0, total_len, CHUNK_SIZE):
            chunk_end = min(chunk_start + CHUNK_SIZE, total_len)
            chunk_data = dataset.select(range(chunk_start, chunk_end))
            chunk_questions = chunk_data['question']
            chunk_answers = chunk_data['answer']
            
            # --- 1. æ‰¹é‡æ£€ç´¢ ---
            q_embeds = self.embedder.encode(chunk_questions).tolist()
            retrieved_results = self.memory.batch_retrieve(q_embeds)
            
            # --- 2. æ„é€  Prompts & è®°å½•ç”¨åˆ°çš„ Skill ---
            inference_prompts = []
            used_rag_data = [] # è®°å½• [(sid, content), None, ...] ç”¨äºæ›´æ–°åˆ†æ•°
            
            for idx, q in enumerate(chunk_questions):
                res = retrieved_results[idx]
                if res and res[1] < 1.0: # è·ç¦»é˜ˆå€¼
                    content, dist, sid = res
                    # åªæœ‰åˆ†æ•°ä¸ç®—å¤ªçƒ‚çš„æ‰ç”¨
                    curr_score = self.memory.skill_stats.get(sid, {}).get("score", 0.5)
                    if curr_score > 0.2:
                        prompt = f"Hint: {content}\nQuestion: {q}\nAnswer step-by-step:"
                        used_rag_data.append((sid, content))
                    else:
                        prompt = f"Question: {q}\nAnswer step-by-step:"
                        used_rag_data.append(None)
                else:
                    prompt = f"Question: {q}\nAnswer step-by-step:"
                    used_rag_data.append(None)
                inference_prompts.append(prompt)
            
            # --- 3. æ‰¹é‡æ¨ç† ---
            model_outputs = []
            for i in range(0, len(inference_prompts), INFERENCE_BATCH_SIZE):
                batch_prompts = inference_prompts[i : i + INFERENCE_BATCH_SIZE]
                batch_outs = self.batch_generate(batch_prompts)
                model_outputs.extend(batch_outs)
            
            # --- 4. è¯„ä¼° ---
            is_correct_list = []
            correct_samples = [] 
            incorrect_indices = []
            
            for idx, pred in enumerate(model_outputs):
                gt = chunk_answers[idx]
                is_right = self.check_answer(pred, gt)
                is_correct_list.append(is_right)
                
                if is_right:
                    correct_samples.append((chunk_questions[idx], gt)) 
                else:
                    incorrect_indices.append(idx)
            
            # >>> æ ¸å¿ƒä¿®æ”¹ï¼šä¼ å…¥ model_outputs ä»¥æ”¯æŒæ—è§‚è€…ä¿æŠ¤ <<<
            self.memory.update_scores_batch(used_rag_data, is_correct_list, model_outputs)

            # --- 5. æ‰¹é‡åæ€ ---
            if incorrect_indices:
                reflect_prompts = []
                # è®°å½•åŸå§‹é—®é¢˜å’Œç­”æ¡ˆï¼Œç”¨äºéªŒè¯
                verify_data = [] 
                
                for idx in incorrect_indices:
                    q = chunk_questions[idx]
                    gt = chunk_answers[idx]
                    p = f"""
I failed this problem.
Problem: {q}
Solution: {gt}
Summarize a general math rule to solve this.
Format:
**Trigger (A)**: [Short Pattern]
**Strategy (B)**: [Short Logic]
"""
                    reflect_prompts.append(p)
                    verify_data.append((q, gt))
                
                # A. æ‰¹é‡ç”Ÿæˆåæ€
                reflections = self.batch_generate(reflect_prompts, temperature=0.7)
                
                # B. è§£æåæ€ç»“æœ
                parsed_patterns, parsed_strategies, valid_indices = [], [], []
                
                # è§£æå‡º pattern å’Œ strategy
                temp_candidates = []
                for k, text in enumerate(reflections):
                    try:
                        if "**Trigger (A)**:" in text and "**Strategy (B)**:" in text:
                            parts = text.split("**Strategy (B)**:")
                            p_text = parts[0].replace("**Trigger (A)**:", "").strip()
                            s_text = parts[1].strip()
                            if len(p_text) > 5 and len(s_text) > 5:
                                temp_candidates.append((p_text, s_text, k))
                    except:
                        continue
                
                # C. >>> æ ¸å¿ƒä¿®æ”¹ï¼šéªŒè¯ç¯èŠ‚ (Verification) <<<
                # æ‹¿åˆšæ‰ç”Ÿæˆçš„â€œç»éªŒâ€ï¼Œç«‹åˆ»å»è¯•ç€è§£ä¸€éåŸé¢˜
                if temp_candidates:
                    verify_prompts = []
                    for p_text, s_text, k in temp_candidates:
                        orig_q = verify_data[k][0]
                        # å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨æ–°ç”Ÿæˆçš„ç»éªŒè§£é¢˜
                        vp = f"Hint: {s_text}\nQuestion: {orig_q}\nAnswer step-by-step:"
                        verify_prompts.append(vp)
                    
                    # æ‰¹é‡éªŒè¯æ¨ç†
                    verify_outputs = self.batch_generate(verify_prompts, temperature=0.1)
                    
                    # åªæœ‰åšå¯¹çš„ï¼Œæ‰å­˜å…¥æ•°æ®åº“ï¼
                    verified_patterns = []
                    verified_strategies = []
                    
                    for m, pred in enumerate(verify_outputs):
                        orig_gt = verify_data[temp_candidates[m][2]][1]
                        if self.check_answer(pred, orig_gt):
                            # ğŸ‰ éªŒè¯é€šè¿‡ï¼è¿™æ¡ç»éªŒæ˜¯æœ‰ç”¨çš„ï¼
                            verified_patterns.append(temp_candidates[m][0])
                            verified_strategies.append(temp_candidates[m][1])
                    
                    # D. å­˜å…¥ç»è¿‡éªŒè¯çš„é«˜è´¨é‡ç»éªŒ
                    if verified_patterns:
                        p_embeds = self.embedder.encode(verified_patterns).tolist()
                        self.memory.add_experience_batch(verified_patterns, verified_strategies, p_embeds)
                        print(f"âœ¨ [éªŒè¯é€šè¿‡] æ–°å¢ {len(verified_patterns)} æ¡æœ‰æ•ˆç»éªŒ (æ·˜æ±°äº† {len(temp_candidates) - len(verified_patterns)} æ¡åƒåœ¾ç»éªŒ)")
                    else:
                        print(f"ğŸ’€ [éªŒè¯å¤±è´¥] ç”Ÿæˆçš„ {len(temp_candidates)} æ¡ç»éªŒå…¨æ˜¯æ— æ•ˆçš„")
                        
            # --- 6. æ‰¹é‡å¾®è°ƒ ---
            loss = 0
            if correct_samples:
                loss = self.train_on_chunk(correct_samples)

            # --- 7. å®šæœŸæ·˜æ±° ---
            pruned_count = 0
            if (chunk_start // CHUNK_SIZE) % 5 == 0:
                pruned_count = self.memory.prune_db(min_usage=5, threshold=0.25)

            acc = len(correct_samples) / len(chunk_data) * 100
            print(f"Chunk {chunk_start//CHUNK_SIZE}: Acc={acc:.1f}% | Pruned={pruned_count} | DB Size={self.memory.collection.count()}")

            if (chunk_start // CHUNK_SIZE) % 5 == 0:
                self.model.save_pretrained("./evolved_qwen_lora_checkpoint")

        print("å…¨é‡è®­ç»ƒå®Œæˆï¼")
        self.model.save_pretrained("./evolved_qwen_lora")

    def extract_number(self, text):
        if not text: return None
        text = text.replace(',', '')
        matches = re.findall(r'-?\d+\.?\d*', text)
        if matches: return float(matches[-1])
        return None

    def check_answer(self, pred, gt):
        if "####" in gt:
            gold = self.extract_number(gt.split("####")[1])
        else:
            gold = self.extract_number(gt)
        pred_num = self.extract_number(pred)
        if gold is None or pred_num is None: return False
        return abs(gold - pred_num) < 1e-4

if __name__ == "__main__":
    trainer = ReflexionTrainerFull()
    trainer.run_full_evolution()