from config_eval import * 
import os
import re
import time
import torch

import chromadb
from collections import Counter
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from vllm import LLM, SamplingParams
from tqdm import tqdm

# ================= 1. è®°å¿†ç®¡ç†å™¨ (ä¿æŒä¸å˜) =================
class MemoryManager:
    def __init__(self):
        self.DB_PATH = DB_PATH
        self.client = chromadb.PersistentClient(path=self.DB_PATH)
        self.collection = self.client.get_collection(name="rule_book")
        
    def batch_retrieve(self, query_embeddings, top_k=3):
        count = self.collection.count()
        if count == 0: return [[] for _ in range(len(query_embeddings))]
        real_k = min(top_k, count)
        results_list = []
        try:
            results = self.collection.query(query_embeddings=query_embeddings, n_results=real_k)
            for i in range(len(query_embeddings)):
                sample_docs = []
                if results['ids'][i]:
                    for j in range(len(results['ids'][i])):
                        doc = results['documents'][i][j]
                        dist = results['distances'][i][j]
                        sample_docs.append((doc, dist))
                results_list.append(sample_docs)
        except:
            return [[] for _ in range(len(query_embeddings))]
        return results_list

# ================= 2. ç§‘å­¦å¯¹æ¯”è¯„ä¼°å™¨ (å®Œæ•´ä¿®æ”¹ç‰ˆ) =================
class ScientificComparator:
    def __init__(self):
        print(f"ğŸš€ åˆå§‹åŒ– vLLM å¼•æ“ (Rigorous Mode)...")
        self.MODEL_PATH = MODEL_NAME
        self.DB_PATH = DB_PATH
        self.GPU_UTILIZATION = GPU_MEMORY_UTILIZATION
        self.TOP_K = TOP_K
        self.SC_PATHS = SC_PATHS 
        # [ä¿®æ”¹] æ”¶ç´§é˜ˆå€¼ï¼Œåªæœ‰éå¸¸åŒ¹é…çš„è§„åˆ™æ‰å¯ç”¨ RAGï¼Œé˜²æ­¢å™ªéŸ³å¹²æ‰°
        self.RAG_THRESHOLD = RAG_THRESHOLD  # å»ºè®®è®¾ä¸º 0.35 æˆ– 0.4ï¼Œè¶Šå°è¶Šä¸¥
        
        self.llm = LLM(
            model=self.MODEL_PATH, 
            trust_remote_code=True,
            gpu_memory_utilization=self.GPU_UTILIZATION,
            tensor_parallel_size=1, 
            max_model_len=2048
        )
        
        # 1. è§£é¢˜ç”¨çš„é‡‡æ ·å‚æ•°
        self.params_sc = SamplingParams(
            n=self.SC_PATHS, 
            temperature=0.3, 
            top_p=0.9, 
            max_tokens=1024,
            stop=["<|endoftext|>", "<|im_end|>", "Question:"]
        )

        # 2. æŠ½è±¡æ„å›¾ç”¨çš„é‡‡æ ·å‚æ•°
        self.params_greedy = SamplingParams(
            temperature=0.0, 
            max_tokens=128,
            stop=["<|endoftext|>", "<|im_end|>", "\n\n"]
        )

        print("ğŸ“¥ åŠ è½½ Embedder (CPU)...")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device="cpu")
        self.memory = MemoryManager()

    def construct_base_prompt(self, question):
        # æ ‡å‡† CoT Prompt
        return f"<|im_start|>user\nQuestion: {question}\nLet's think step by step.\nAnswer:<|im_end|>\n<|im_start|>assistant\n"

    def construct_rag_prompt(self, question, retrieved_items):
        valid_items = [item[0] for item in retrieved_items if item[1] < self.RAG_THRESHOLD]
        
        # [é€»è¾‘ä¿æŠ¤] å¦‚æœè¿‡æ»¤åæ²¡æœ‰è§„åˆ™ï¼Œè¿”å› Noneï¼ŒæŒ‡ç¤ºè°ƒç”¨è€…ä½¿ç”¨ Base ç»“æœ
        if not valid_items:
            return None
        
        context_str = "\n".join([f"[Rule {i+1}]: {rule}" for i, rule in enumerate(valid_items)])
        
        # [å…³é”®ä¿®æ”¹] 
        # 1. è¯­æ°”å˜è½¯ï¼šReference Rules (Only if helpful)
        # 2. æ ¸å¿ƒä¿®å¤ï¼šå¥å°¾åŠ å› "Let's think step by step." æ¿€æ´»æ¨¡å‹æ™ºå•†
        prompt = f"""<|im_start|>user
[Reference Rules (Use ONLY if helpful)]
{context_str}

[Question]
{question}

Let's think step by step using the rules above if applicable.
Answer:<|im_end|>
<|im_start|>assistant
"""
        return prompt

    def extract_answer(self, text):
        if not text: return None
        text = text.replace(',', '')
        matches = re.findall(r'-?\d+\.?\d*', text)
        if matches: return float(matches[-1])
        return None

    def majority_vote(self, request_output):
        valid_nums = []
        for output in request_output.outputs:
            num = self.extract_answer(output.text)
            if num is not None:
                valid_nums.append(num)
        if not valid_nums: return None
        return Counter(valid_nums).most_common(1)[0][0]

    def check_correct(self, pred, gt_str):
        if "####" in gt_str:
            gold = self.extract_answer(gt_str.split("####")[1])
        else:
            gold = self.extract_answer(gt_str)
        if pred is None or gold is None: return False
        return abs(pred - gold) < 1e-4

    def batch_abstract_for_retrieval(self, questions):
        # ... (ä¿æŒåŸæ¥çš„æŠ½è±¡å‡½æ•°ä¸å˜) ...
        prompts = []
        for q in questions:
            content = f"""Task: Identify the core mathematical concept. Output 1 abstract sentence without numbers.
[Example] Q: John has 5 apples... A: Calculating total sum.
[Target] Q: {q}
A:"""
            prompts.append(f"<|im_start|>user\n{content}<|im_end|>\n<|im_start|>assistant\n")
        
        print("   ğŸ§  Abstracting questions...")
        outputs = self.llm.generate(prompts, self.params_greedy, use_tqdm=True)
        return [output.outputs[0].text.strip() for output in outputs]

    # ================= [æ ¸å¿ƒä¿®æ”¹] è¯„ä¼°ä¸»å¾ªç¯ =================
    def run_scientific_test(self):
        dataset = load_dataset("gsm8k", "main")['test']
        questions = dataset['question']
        ground_truths = dataset['answer']
        total = len(questions)
        
        print(f"ğŸ“Š æµ‹è¯•é›†å¤§å°: {total} | SC={self.SC_PATHS} | RAG Threshold={self.RAG_THRESHOLD}")

        # ------------------------------------------------------------------
        # Phase 1: Base Model (ä¿å­˜ Base ç»“æœç”¨äºå›é€€ï¼)
        # ------------------------------------------------------------------
        print(f"\nğŸ”µ [Group A] Base Model...")
        base_prompts = [self.construct_base_prompt(q) for q in questions]
        
        # è¿è¡Œ Base æ¨ç†
        t0 = time.time()
        base_outputs_obj = self.llm.generate(base_prompts, self.params_sc, use_tqdm=True)
        print(f"   Base è€—æ—¶: {time.time()-t0:.2f}s")

        # è®¡ç®—å¹¶ç¼“å­˜ Base çš„ç»“æœ
        base_predictions = []
        correct_base = 0
        for i, out in enumerate(base_outputs_obj):
            pred = self.majority_vote(out)
            base_predictions.append(pred) # å­˜èµ·æ¥ï¼
            if self.check_correct(pred, ground_truths[i]):
                correct_base += 1
        
        acc_base = correct_base / total * 100
        print(f"   âœ… Base Accuracy: {acc_base:.2f}%")

        # ------------------------------------------------------------------
        # Phase 2: Hybrid RAG Model (Fallback Logic)
        # ------------------------------------------------------------------
        print(f"\nğŸŸ¢ [Group B] Hybrid RAG (Recall & Fallback)...")
        print("   ç­–ç•¥ï¼šä»…å½“æ£€ç´¢åˆ°çš„ç»éªŒæå…¶åŒ¹é…æ—¶(Distance < Threshold)æ‰å¯ç”¨ RAGï¼Œå¦åˆ™ç›´æ¥å¤ç”¨ Base ç­”æ¡ˆã€‚")
        
        # 1. æŠ½è±¡ + æ£€ç´¢
        abstract_queries = self.batch_abstract_for_retrieval(questions)
        print("   -> Encoding & Retrieving...")
        abstract_embeddings = self.embedder.encode(abstract_queries, batch_size=64, convert_to_numpy=True).tolist()
        all_retrieved = self.memory.batch_retrieve(abstract_embeddings, top_k=self.TOP_K)
        
        # 2. æ„å»ºæ··åˆä»»åŠ¡åˆ—è¡¨
        rag_prompts = []
        rag_indices = [] # è®°å½•å“ªäº›é¢˜ç›®éœ€è¦è·‘ RAG
        final_rag_preds = [None] * total # é¢„å¡«å……åˆ—è¡¨
        
        skipped_count = 0
        
        for i, q in enumerate(questions):
            # å°è¯•æ„å»º RAG promptï¼Œå¦‚æœè·ç¦»å¤ªè¿œï¼Œconstruct_rag_prompt ä¼šè¿”å› None
            prompt = self.construct_rag_prompt(q, all_retrieved[i])
            
            if prompt is None:
                # [å›é€€é€»è¾‘]ï¼šç»éªŒä¸å¯é ï¼Œç›´æ¥å¤ç”¨ Base çš„é¢„æµ‹ç»“æœï¼
                # è¿™æ ·å¯ä»¥ä¿è¯å‡†ç¡®ç‡ç»å¯¹ä¸ä¼šå› ä¸ºâ€œå¼ºè¡ŒRAGâ€è€Œä½äº Base (é™¤é RAG æŠŠåŸæœ¬å¯¹çš„æ”¹é”™äº†)
                final_rag_preds[i] = base_predictions[i]
                skipped_count += 1
            else:
                # ç»éªŒå¯é ï¼ŒåŠ å…¥é‡ç®—é˜Ÿåˆ—
                rag_prompts.append(prompt)
                rag_indices.append(i)

        print(f"   â„¹ï¸  RAG è§¦å‘ç‡: {len(rag_indices)}/{total} (Fallback to Base: {skipped_count})")

        # 3. åªå¯¹è§¦å‘äº† RAG çš„é¢˜ç›®è¿›è¡Œæ¨ç† (èŠ‚çœå¤§é‡æ—¶é—´ï¼)
        if rag_prompts:
            print(f"   ğŸš€ Running RAG Inference on {len(rag_prompts)} samples...")
            t0 = time.time()
            rag_inference_outputs = self.llm.generate(rag_prompts, self.params_sc, use_tqdm=True)
            print(f"   RAG éƒ¨åˆ†è€—æ—¶: {time.time()-t0:.2f}s")
            
            # å¡«å›ç»“æœ
            for idx_in_batch, out in enumerate(rag_inference_outputs):
                original_idx = rag_indices[idx_in_batch]
                pred = self.majority_vote(out)
                final_rag_preds[original_idx] = pred
        
        # 4. ç»Ÿè®¡ Group B æœ€ç»ˆç»“æœ
        correct_rag = 0
        for i, pred in enumerate(final_rag_preds):
            if self.check_correct(pred, ground_truths[i]):
                correct_rag += 1
        
        acc_rag = correct_rag / total * 100
        print(f"   âœ… Hybrid RAG Accuracy: {acc_rag:.2f}%")

        # ================= æœ€ç»ˆåˆ†æ =================
        print("\n" + "="*60)
        print("ğŸ§ª æœ€ç»ˆæŠ¥å‘Š")
        print("="*60)
        print(f"1. Base Model Acc  : {acc_base:.2f}%")
        print(f"2. Hybrid RAG Acc  : {acc_rag:.2f}%")
        print("-" * 60)
        diff = acc_rag - acc_base
        print(f"ğŸ“ˆ å‡€æå‡: {diff:+.2f}%")
        
        if diff >= 0:
            print("ç»“è®ºï¼šæ··åˆç­–ç•¥ç”Ÿæ•ˆã€‚ç³»ç»Ÿä¿ç•™äº†åŸºåº§èƒ½åŠ›ï¼Œå¹¶åœ¨æœ‰ç»éªŒæ—¶è·å¾—äº†å¢ç›Šã€‚")
        else:
            print("ç»“è®ºï¼šä»ç„¶æœ‰ä¸‹é™ï¼Ÿè¯·æ£€æŸ¥ RAG Prompt æ˜¯å¦å¹²æ‰°äº†æ¨¡å‹ã€‚")
        print("="*60)


if __name__ == "__main__":

    
    try:
        evaluator = ScientificComparator()
        evaluator.run_scientific_test()
    except KeyboardInterrupt:
        print("\nğŸ›‘ è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°å‡ºé”™: {e}")